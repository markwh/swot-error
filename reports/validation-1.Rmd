---
title: "validation-phase 1"
author: "Mark Hagemann"
date: "4/5/2019"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(plotly)
library(leaflet)

opts_chunk$set(echo = FALSE, cache = TRUE, cache.rebuild = TRUE)
opts_knit$set(root.dir = "~/Documents/swot-error")
```

```{r, message=FALSE, warning =FALSE}
library(ProjectTemplate)
load.project()

devtools::load_all("../rivertile")
theme_set(theme_bw())

nodeval109 <- rt_valdata(rodir(14))
nodeval220 <- rt_valdata(rodir(18))
nodevaldf <- list(`109` = nodeval109, `220` = nodeval220) %>% 
  bind_rows(.id = "day") %>% 
  mutate(day = as.numeric(day))

reachval109 <- rt_valdata(rodir(14), group = "reaches")
reachval220 <- rt_valdata(rodir(18), group = "reaches")
reachvaldf <- list(`109` = reachval109, `220` = reachval220) %>% 
  bind_rows(.id = "day") %>% 
  mutate(day = as.numeric(day))

nodelalo <- nodeval109 %>% 
  dplyr::select(node_id, reach_id, gdem_val, variable) %>% 
  dplyr::filter(variable %in% c("latitude", "longitude")) %>% 
  spread(key = variable, value = gdem_val)

valvars <- c("height", "height2", "width", "slope", "area_total")



```


This report presents the results of "phase 1" of validating uncertainty estimates using two different days of simulations on the Sacramento River. Phases 2 and 3 will separate results depending on variables such as uncertainty magnitude and feature size; phase 1 treats all results uniformly, thereby establishing a simple yes/no answer to the question of whether predicted uncertainty matches empirical errors across the entire validation set. 

## Dataset

```{r, eval = FALSE}
length(unique(nodelalo$node_id))
length(unique(nodelalo$reach_id))
min(nodelalo$latitude)
max(nodelalo$latitude)
```

The study area consists of 365 nodes in comprising 6 reaches in the Sacramento River between 39.12 and 39.55 degrees latitude. Two simulations of the SWOT single-look complex (SLC) were created using two different flow conditions; other simulation parameters including $\sigma_0$ for water and land, SWOT pass number, and smearing distance were held constand. The water and land $\sigma_0$ were selected so as to remove any impact of layover.  

```{r}
reachids <- sort(unique(nodelalo$reach_id))
nreaches <- length(reachids)

dkcols <- RColorBrewer::brewer.pal(n = 8, name = "Set3")
reachpal <- colorFactor(palette = rep(dkcols, length.out = nreaches), 
                         domain = reachids)

leaflet(nodelalo) %>% 
  addTiles() %>% 
  addCircleMarkers(radius = 2, color = ~reachpal(reach_id), 
                   popup = ~paste(sprintf("reach: %s\nnode: %s", 
                                          reach_id, node_id)), 
                   opacity = 1) %>% 
  addLegend(position = "topright", pal = reachpal, values = reachids, 
            title = "Reach ID", opacity = 1)
```



## Node Results

Node variables validated include height, height2, width, and total area. The "height2" variable differs from "height" only in its uncertainty estimate (using a different method to estimate uncertainty).

The results in phase 1 are presented in 3 forms: Chi-square hypothesis tests of empirical variance, plots comparing theoretical to empirical distributions, and tables comparing theoretical to empirical statistics. Results at the node scale are presented in section ####; reach-scale results are presented in section ####. 

### Hypothesis tests

In the combined validation data from days 109 and 220, 2-sided chi-square hypothesis tests rejected the null hypothesis ($\sigma = \hat{\sigma}$) for all node variables at a significance level of 0.05. Even when errors were bias-adjusted the results of the hypothesis test are the same (Table ####). This indicates that empirical variance was significantly different from modeled variance. However, the hypothesis test for height was only marginally significant ($p = 0.035$ and $0.043$ for non-adjusted and bias-adjusted, respectively), whereas all other variables had highly significant hypothesis tests ($p < 10^{-6}$).

```{r}
node_ht2 <- nodevaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  # dplyr::mutate(sigma_est = ifelse(variable == "area_total", 
  #                                  sigma_est * 2.36, sigma_est)) %>% 
  rt_hyptest(sides = 2)

node_ht2_db <- nodevaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  rt_hyptest(debias = TRUE)

node_ht2$pval_debias <- node_ht2_db$pval

kable(node_ht2, format.args = list(scientific = -2))

```

Restricting the validation to include only the lower-flow condition (day 109) gives a different result for height; here the test does not reject the null hypthesis $\sigma = \hat{\sigma}$ at a significance level of 0.05. All other variables have error variance that is again significantly different from the estimated variance. 

```{r}
node_ht109 <- nodevaldf %>% 
  dplyr::filter(day == 109, variable %in% valvars) %>% 
  rt_hyptest(sides = 2)

node_ht109_db <- nodevaldf %>% 
  dplyr::filter(day == 109, variable %in% valvars) %>% 
  rt_hyptest(debias = TRUE)

node_ht109$pval_debias <- node_ht109_db$pval

kable(node_ht109, format.args = list(scientific = -2))

```


### Distribution plots

Histograms for node-level scaled errors (Fig. ####) illustrate the difference between the empirical (histogram) and theoretical (smooth curve) error distributions. Area and width (a nodewise constant scaling of area) have heavy upper tails, especially for the high-flow day 220, indicating a tendency to vastly overestimate node areas in some cases. Three of the worst overestimates (nodes 286, 301, and 360) are mapped below, and their aggregation of pixels compared to the validation truth in figure ####.

```{r}
ambiguous_nodes(rodir(18))
histdf <- rt_val_hist(nodevaldf, scale = TRUE, plot = FALSE)

histgg1 <- histdf %>%     
  dplyr::filter(variable %in% valvars) %>% 
  ggplot(aes(x = err)) +
  geom_histogram(aes(y = ..density.., fill = as.factor(day), 
                     group = day), position = "dodge",
                 bins = 15) +
  facet_wrap(~variable, scales = "free") +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
  scale_fill_brewer(palette = "Set1")

histgg1
# ggplotly(histgg1)
```


```{r}
# histdf %>% 
#   dplyr::filter(variable == "area_total") %>% 
#   arrange(desc(err)) %>% head()
badnodes <- c(286, 360, 301)
val_map_node(rodir(18), nodes = badnodes, maxpixels = 5000, 
             pcv2 = "pcv_gdem_dil2.nc")

nodearea_plot(pixc_joined = join_pixc(rodir(18)), nodes = badnodes, 
              node_truth = rt_read(fs::path(rodir(18), "rt_gdem.nc")))
```

Normal quantile-quantile plots (Fig. ####) illustrate the deviation of emprical scaled errors from the theoretical $N(0, 1)$ distribution (solid black line). While the height errors closely match the theoretical distribution for day 109, day 220 slightly but consistently overestimates large height errors, enough to cause the hypothesis test to reject the equality of theoretical and empirical variance. Height2 scaled errors are apparently Gaussian (falling on a straight line in the QQ plot), but have a larger variance than is predicted (slope of a line through the scaled errors is greater than 1). Width and area scaled errors diverge considerably from the theoretical distribution, and the behavior is markedly different between days 109 and 220. Both days' scaled errors diverge from the theoretical $N(0,1)$ line, but whereas Day 220 has a heavy upper tail, day 109 appears to be Gaussian, but with a larger variance than predicted. 

```{r}

qqgg1 <- nodevaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  group_by(variable, day) %>% 
  mutate(rel_err = pixc_err / sigma_est, 
         theoretical = qqnorm(rel_err, plot.it = FALSE)$x) %>% 
  ungroup() %>% 
  ggplot(aes(x = theoretical, y = rel_err)) + 
  geom_point(aes(text = node_id, color = as.factor(day))) +
  facet_wrap(~variable, scales = "free_y") +
  geom_abline(slope = 1, intercept = 0) +
  scale_color_brewer(palette = "Set1")


# qqgg1
ggplotly(qqgg1, tooltip = "text")
```

```{r}
scattergg1 <- rt_val_scatter(nodevaldf, variables = valvars, yvar = "relerr", plot = FALSE) %>% 
  ggplot(aes(x = xval, y = yval)) +
  # Add ribbons
  geom_ribbon(ymin = -1.96, ymax = 1.96, fill = "#dddddd") +
  geom_ribbon(ymin = -1, ymax = 1, fill = "#aaaaaa") +
  # add points
  geom_point(aes(color = as.factor(day)), alpha = 0.6) +
    facet_wrap(~variable, scales = "free") +
    ylab("relative error") + xlab("Node ID") +
  scale_color_brewer(palette = "Set1")

ggplotly(scattergg1)

```

### Tables 

Coverage rates for various confidence intervals (Table ####) again reflect the underestimation of variance in the uncertainty models. Only the 99% confidence interval for height contains the expected amount (99%) of the data; all other intervals contain less than the theoretical amount. 

```{r}
nodevaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  val_coverage() %>% 
  kable()
```

Summary statistics (Table ####) indicate exactly by how much empirical statistics (bias, standard deviation, RMSE), differ from theoretical values (0, 1, and 1, respectively). Area and width $\hat{\sigma}$'s would need to be multiplied by a factor of 2.36 in order to match the theoretical variance (including bias), whereas $\hat{\sigma}$ for height and height2 would require smaller adjustments. 

```{r}
nodevaldf %>% 
  mutate(relerr = pixc_err / sigma_est) %>% 
  dplyr::filter(variable %in% valvars) %>% 
  group_by(variable) %>% 
  summarize(bias = mean(relerr), sd = sd(relerr)) %>% 
  mutate(rmse = sqrt(bias^2 + sd^2)) %>% 
  kable()
```


## Reach results 



### Plots

```{r}
reachvaldf %>% 
  rt_val_scatter(variables = valvars, yvar = "relerr", plot = FALSE) %>% 
  ggplot(aes(x = xval, y = yval)) +
  # Add ribbons
  geom_ribbon(ymin = -1.96, ymax = 1.96, fill = "#dddddd") +
  geom_ribbon(ymin = -1, ymax = 1, fill = "#aaaaaa") +
  # add points
  geom_point(aes(color = as.factor(day)), alpha = 0.6) +
    facet_wrap(~variable, scales = "free") +
    ylab("relative error") + xlab("Reach ID") +
  scale_color_brewer(palette = "Set1")

```

```{r}

reachht <- reachvaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  rt_hyptest()

reachht_db <- reachvaldf %>% 
  dplyr::filter(variable %in% valvars) %>% 
  rt_hyptest(debias = TRUE)

reachht$pval_debias <- reachht_db$pval

kable(reachht)
```


```{r}

reachht109 <- reachvaldf %>% 
  dplyr::filter(day == 109, variable %in% valvars) %>% 
  rt_hyptest()

reachht109_db <- reachvaldf %>% 
  dplyr::filter(day == 109, variable %in% valvars) %>% 
  rt_hyptest(debias = TRUE)

reachht109$pval_debias <- reachht109_db$pval

kable(reachht109)
```

